{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 4.5523645984630665\n",
      "Random Forest RMSE: 3.2579872712888616\n",
      "Gradient Boosting RMSE: 2.9079805878194303\n",
      "AdaBoost RMSE: 3.8548840050706428\n",
      "Bagging RMSE: 3.2343899960744658\n",
      "Stacking RMSE: 2.9046628135346033\n",
      "Voting RMSE: 3.151664846645677\n",
      "Linear Regression RMAE: 3.1482557548168324\n",
      "Random Forest RMAE: 2.2318023952095802\n",
      "Gradient Boosting RMAE: 1.9814280516443268\n",
      "AdaBoost RMAE: 2.6872865513825976\n",
      "Bagging RMAE: 2.2179640718562874\n",
      "Stacking RMAE: 1.9676701388742954\n",
      "Voting RMAE: 2.1348881014622734\n",
      "Random Forest r2_score: 0.8528390975280444\n",
      "Gradient Boosting r2_score: 0.8637280551774674\n",
      "AdaBoost r2_score: 0.8000608867817437\n",
      "Bagging r2_score: 0.8536096698379272\n",
      "Stacking r2_score: 0.8590508511333066\n",
      "Voting r2_score: 0.8557093740727252\n",
      "Linear r2_score: 0.7136663740773465\n"
     ]
    }
   ],
   "source": [
    "# import required libraries and load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load the Dataset\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.Series(boston.target)\n",
    "\n",
    "# rescale the features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# apply scaler() to all the numeric columns \n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Linear Regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# initialize the ensemble regression models\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "ab = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "bg = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# stack the models and define the meta-regressor\n",
    "stack = StackingRegressor(estimators=[('rf', rf), ('gb', gb), ('ab', ab), ('bg', bg)], final_estimator=lr)\n",
    "\n",
    "# define the voting regressor\n",
    "vote = VotingRegressor(estimators=[('rf', rf), ('gb', gb), ('ab', ab), ('bg', bg)])\n",
    "\n",
    "# fit the models on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "gb.fit(X_train, y_train)\n",
    "ab.fit(X_train, y_train)\n",
    "bg.fit(X_train, y_train)\n",
    "stack.fit(X_train, y_train)\n",
    "vote.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "rf_pred = rf.predict(X_test)\n",
    "gb_pred = gb.predict(X_test)\n",
    "ab_pred = ab.predict(X_test)\n",
    "bg_pred = bg.predict(X_test)\n",
    "stack_pred = stack.predict(X_test)\n",
    "vote_pred = vote.predict(X_test)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# calculate the root mean squared error of each model\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "ab_rmse = np.sqrt(mean_squared_error(y_test, ab_pred))\n",
    "bg_rmse = np.sqrt(mean_squared_error(y_test, bg_pred))\n",
    "stack_rmse = np.sqrt(mean_squared_error(y_test, stack_pred))\n",
    "vote_rmse = np.sqrt(mean_squared_error(y_test, vote_pred))\n",
    "\n",
    "# calculate the root mean absolute error of each model\n",
    "lr_rmae = (mean_absolute_error(y_test, lr_pred))\n",
    "rf_rmae = (mean_absolute_error(y_test, rf_pred))\n",
    "gb_rmae = (mean_absolute_error(y_test, gb_pred))\n",
    "ab_rmae = (mean_absolute_error(y_test, ab_pred))\n",
    "bg_rmae = (mean_absolute_error(y_test, bg_pred))\n",
    "stack_rmae = (mean_absolute_error(y_test, stack_pred))\n",
    "vote_rmae = (mean_absolute_error(y_test, vote_pred))\n",
    "\n",
    "\n",
    "# print the RMSE of each model\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Random Forest RMSE:\", rf_rmse)\n",
    "print(\"Gradient Boosting RMSE:\", gb_rmse)\n",
    "print(\"AdaBoost RMSE:\", ab_rmse)\n",
    "print(\"Bagging RMSE:\", bg_rmse)\n",
    "print(\"Stacking RMSE:\", stack_rmse)\n",
    "print(\"Voting RMSE:\", vote_rmse)\n",
    "\n",
    "\n",
    "# print the RMAE of each model\n",
    "print(\"Linear Regression RMAE:\", lr_rmae)\n",
    "print(\"Random Forest RMAE:\", rf_rmae)\n",
    "print(\"Gradient Boosting RMAE:\", gb_rmae)\n",
    "print(\"AdaBoost RMAE:\", ab_rmae)\n",
    "print(\"Bagging RMAE:\", bg_rmae)\n",
    "print(\"Stacking RMAE:\", stack_rmae)\n",
    "print(\"Voting RMAE:\", vote_rmae)\n",
    "\n",
    "# create a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "# perform cross-validation and calculate the r2_score for each model\n",
    "rf_scores = cross_val_score(rf, X, y, cv=folds, scoring='r2')\n",
    "gb_scores = cross_val_score(gb, X, y, cv=folds, scoring='r2')\n",
    "ab_scores = cross_val_score(ab, X, y, cv=folds, scoring='r2')\n",
    "bg_scores = cross_val_score(bg, X, y, cv=folds, scoring='r2')\n",
    "stack_scores = cross_val_score(stack, X, y, cv=folds, scoring='r2')\n",
    "vote_scores = cross_val_score(vote, X, y, cv=folds, scoring='r2')\n",
    "lr_scores = cross_val_score(lr, X, y, cv=folds, scoring='r2')\n",
    "\n",
    "# print the r2_score of each model\n",
    "print(\"Random Forest r2_score:\", np.mean(rf_scores))\n",
    "print(\"Gradient Boosting r2_score:\", np.mean(gb_scores))\n",
    "print(\"AdaBoost r2_score:\", np.mean(ab_scores))\n",
    "print(\"Bagging r2_score:\", np.mean(bg_scores))\n",
    "print(\"Stacking r2_score:\", np.mean(stack_scores))\n",
    "print(\"Voting r2_score:\", np.mean(vote_scores))\n",
    "print(\"Linear r2_score:\", np.mean(lr_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
