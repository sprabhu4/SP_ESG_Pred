{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Linear Regression r2_score: 0.7406426641094095\n",
      "Random Forest Regression best parameters: {'max_depth': 10, 'n_estimators': 200}\n",
      "Random Forest Regression r2_score: 0.6285031509753464\n",
      "Gradient Boosting Regression best parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Gradient Boosting Regression r2_score: 0.5580127580938832\n",
      "AdaBoost Regression best parameters: {'learning_rate': 1.0, 'n_estimators': 100}\n",
      "AdaBoost Regression r2_score: 0.608589752644294\n",
      "Bagging Regression best parameters: {'max_samples': 0.8, 'n_estimators': 200}\n",
      "Bagging Regression r2_score: 0.6394893326342523\n",
      "Stacking Regression best parameters: {'final_estimator__fit_intercept': True, 'final_estimator__normalize': False}\n",
      "Stacking Regression r2_score: 0.6346440505800522\n",
      "Voting Regression best parameters: {'weights': [1, 1, 2, 1, 1]}\n",
      "Voting Regression r2_score: 0.6654805878995067\n"
     ]
    }
   ],
   "source": [
    "# import required libraries and load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.Series(boston.target)\n",
    "\n",
    "# initialize the regression models\n",
    "lr = LinearRegression()\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "ab = AdaBoostRegressor(random_state=42)\n",
    "bg = BaggingRegressor(random_state=42)\n",
    "# stack the models and define the meta-regressor\n",
    "stack = StackingRegressor(estimators=[('rf', rf), ('gb', gb), ('ab', ab), ('bg', bg)], final_estimator=lr)\n",
    "\n",
    "# define the voting regressor\n",
    "vote = VotingRegressor(estimators=[('lr', lr), ('rf', rf), ('gb', gb), ('ab', ab), ('bg', bg)])\n",
    "\n",
    "# define the hyperparameter grid for each model\n",
    "rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15]}\n",
    "gb_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "ab_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "bg_param_grid = {'n_estimators': [50, 100, 200], 'max_samples': [0.5, 0.8, 1.0]}\n",
    "stack_param_grid = {\n",
    "    'final_estimator__fit_intercept': [True, False],\n",
    "    'final_estimator__normalize': [True, False]\n",
    "}\n",
    "\n",
    "vote_param_grid = {\n",
    "    'weights': [[1, 1, 1, 1, 1], [1, 2, 1, 1, 1], [1, 1, 2, 1, 1], [1, 1, 1, 2, 1], [1, 1, 1, 1, 2]],\n",
    "}\n",
    "# perform hyperparameter tuning using GridSearchCV for each model\n",
    "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='r2')\n",
    "rf_grid.fit(X, y)\n",
    "\n",
    "gb_grid = GridSearchCV(gb, gb_param_grid, cv=5, scoring='r2')\n",
    "gb_grid.fit(X, y)\n",
    "\n",
    "ab_grid = GridSearchCV(ab, ab_param_grid, cv=5, scoring='r2')\n",
    "ab_grid.fit(X, y)\n",
    "\n",
    "bg_grid = GridSearchCV(bg, bg_param_grid, cv=5, scoring='r2')\n",
    "bg_grid.fit(X, y)\n",
    "\n",
    "stack_grid = GridSearchCV(stack, stack_param_grid, cv=5, scoring='r2')\n",
    "stack_grid.fit(X, y)\n",
    "\n",
    "vote_grid = GridSearchCV(vote, vote_param_grid, cv=5, scoring='r2')\n",
    "vote_grid.fit(X, y)\n",
    "# print the best parameters and r2_score of each model after hyperparameter tuning\n",
    "print(\"Simple Linear Regression r2_score:\", r2_score(y, lr.fit(X, y).predict(X)))\n",
    "print(\"Random Forest Regression best parameters:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Regression r2_score:\", rf_grid.best_score_)\n",
    "print(\"Gradient Boosting Regression best parameters:\", gb_grid.best_params_)\n",
    "print(\"Gradient Boosting Regression r2_score:\", gb_grid.best_score_)\n",
    "print(\"AdaBoost Regression best parameters:\", ab_grid.best_params_)\n",
    "print(\"AdaBoost Regression r2_score:\", ab_grid.best_score_)\n",
    "print(\"Bagging Regression best parameters:\", bg_grid.best_params_)\n",
    "print(\"Bagging Regression r2_score:\", bg_grid.best_score_)\n",
    "print(\"Stacking Regression best parameters:\", stack_grid.best_params_)\n",
    "print(\"Stacking Regression r2_score:\", stack_grid.best_score_)\n",
    "print(\"Voting Regression best parameters:\", vote_grid.best_params_)\n",
    "print(\"Voting Regression r2_score:\", vote_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries and load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load the Dataset and fill the X and Y values\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.Series(boston.target)\n",
    "\n",
    "# rescale the features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# apply scaler() to all the numeric columns \n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Linear Regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# initialize the ensemble regression models\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "ab = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "bg = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# stack the models and define the meta-regressor\n",
    "stack = StackingRegressor(estimators=[('rf', rf), ('gb', gb), ('ab', ab), ('bg', bg)], final_estimator=lr)\n",
    "\n",
    "# define the voting regressor\n",
    "vote = VotingRegressor(estimators=[('rf', rf), ('gb', gb), ('ab', ab), ('bg', bg)])\n",
    "\n",
    "#######################################################\n",
    "# define the hyperparameter grid for each model\n",
    "rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15]}\n",
    "gb_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 15], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "ab_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.5, 1.0]}\n",
    "bg_param_grid = {'n_estimators': [50, 100, 200], 'max_samples': [0.5, 0.8, 1.0]}\n",
    "stack_param_grid = {\n",
    "    'final_estimator__fit_intercept': [True, False],\n",
    "    'final_estimator__normalize': [True, False]\n",
    "}\n",
    "\n",
    "vote_param_grid = {\n",
    "    'weights': [[1, 1, 1, 1, 1], [1, 2, 1, 1, 1], [1, 1, 2, 1, 1], [1, 1, 1, 2, 1], [1, 1, 1, 1, 2]],\n",
    "}\n",
    "# perform hyperparameter tuning using GridSearchCV for each model\n",
    "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "gb_grid = GridSearchCV(gb, gb_param_grid, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "ab_grid = GridSearchCV(ab, ab_param_grid, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "bg_grid = GridSearchCV(bg, bg_param_grid, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "stack_grid = GridSearchCV(stack, stack_param_grid, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "vote_grid = GridSearchCV(vote, vote_param_grid, cv=5, scoring='r2')\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# fit the models on the training data\n",
    "rf_grid.fit(X_train, y_train)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "ab_grid.fit(X_train, y_train)\n",
    "bg_grid.fit(X_train, y_train)\n",
    "stack_grid.fit(X_train, y_train)\n",
    "vote_grid.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "rf_pred = rf_grid.predict(X_test)\n",
    "gb_pred = gb_grid.predict(X_test)\n",
    "ab_pred = ab_grid.predict(X_test)\n",
    "bg_pred = bg_grid.predict(X_test)\n",
    "stack_pred = stack_grid.predict(X_test)\n",
    "vote_pred = vote_grid.predict(X_test)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# calculate the root mean squared error of each model\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "ab_rmse = np.sqrt(mean_squared_error(y_test, ab_pred))\n",
    "bg_rmse = np.sqrt(mean_squared_error(y_test, bg_pred))\n",
    "stack_rmse = np.sqrt(mean_squared_error(y_test, stack_pred))\n",
    "vote_rmse = np.sqrt(mean_squared_error(y_test, vote_pred))\n",
    "\n",
    "# calculate the root mean absolute error of each model\n",
    "lr_rmae = (mean_absolute_error(y_test, lr_pred))\n",
    "rf_rmae = (mean_absolute_error(y_test, rf_pred))\n",
    "gb_rmae = (mean_absolute_error(y_test, gb_pred))\n",
    "ab_rmae = (mean_absolute_error(y_test, ab_pred))\n",
    "bg_rmae = (mean_absolute_error(y_test, bg_pred))\n",
    "stack_rmae = (mean_absolute_error(y_test, stack_pred))\n",
    "vote_rmae = (mean_absolute_error(y_test, vote_pred))\n",
    "\n",
    "\n",
    "# print the RMSE of each model\n",
    "print(\"Linear Regression RMSE:\", lr_rmse)\n",
    "print(\"Random Forest RMSE:\", rf_rmse)\n",
    "print(\"Gradient Boosting RMSE:\", gb_rmse)\n",
    "print(\"AdaBoost RMSE:\", ab_rmse)\n",
    "print(\"Bagging RMSE:\", bg_rmse)\n",
    "print(\"Stacking RMSE:\", stack_rmse)\n",
    "print(\"Voting RMSE:\", vote_rmse)\n",
    "\n",
    "\n",
    "# print the RMAE of each model\n",
    "print(\"Linear Regression RMAE:\", lr_rmae)\n",
    "print(\"Random Forest RMAE:\", rf_rmae)\n",
    "print(\"Gradient Boosting RMAE:\", gb_rmae)\n",
    "print(\"AdaBoost RMAE:\", ab_rmae)\n",
    "print(\"Bagging RMAE:\", bg_rmae)\n",
    "print(\"Stacking RMAE:\", stack_rmae)\n",
    "print(\"Voting RMAE:\", vote_rmae)\n",
    "\n",
    "# create a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "# perform cross-validation and calculate the r2_score for each model\n",
    "rf_scores = cross_val_score(rf, X, y, cv=folds, scoring='r2')\n",
    "gb_scores = cross_val_score(gb, X, y, cv=folds, scoring='r2')\n",
    "ab_scores = cross_val_score(ab, X, y, cv=folds, scoring='r2')\n",
    "bg_scores = cross_val_score(bg, X, y, cv=folds, scoring='r2')\n",
    "stack_scores = cross_val_score(stack, X, y, cv=folds, scoring='r2')\n",
    "vote_scores = cross_val_score(vote, X, y, cv=folds, scoring='r2')\n",
    "lr_scores = cross_val_score(lr, X, y, cv=folds, scoring='r2')\n",
    "\n",
    "# print the r2_score of each model\n",
    "print(\"Random Forest Regression best parameters:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Regression r2_score:\", rf_grid.best_score_)\n",
    "print(\"Gradient Boosting Regression best parameters:\", gb_grid.best_params_)\n",
    "print(\"Gradient Boosting Regression r2_score:\", gb_grid.best_score_)\n",
    "print(\"AdaBoost Regression best parameters:\", ab_grid.best_params_)\n",
    "print(\"AdaBoost Regression r2_score:\", ab_grid.best_score_)\n",
    "print(\"Bagging Regression best parameters:\", bg_grid.best_params_)\n",
    "print(\"Bagging Regression r2_score:\", bg_grid.best_score_)\n",
    "print(\"Stacking Regression best parameters:\", stack_grid.best_params_)\n",
    "print(\"Stacking Regression r2_score:\", stack_grid.best_score_)\n",
    "print(\"Voting Regression best parameters:\", vote_grid.best_params_)\n",
    "print(\"Voting Regression r2_score:\", vote_grid.best_score_)\n",
    "print(\"Linear r2_score:\", np.mean(lr_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
